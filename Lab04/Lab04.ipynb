{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lab 04:\n",
        "Nguyễn Hữu Quyến - 19522113\n",
        "link: \n"
      ],
      "metadata": {
        "id": "K7KsD0qnKifS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN_r0L2oE2Li"
      },
      "outputs": [],
      "source": [
        "# import necessary modules\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = datasets.load_digits()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the image 1010\n",
        "plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OLsTGqtSG2of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# split into trainning and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify=y)"
      ],
      "metadata": {
        "id": "Gw2XX48jH6dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build KNN classifier for the above dataset."
      ],
      "metadata": {
        "id": "lti4VuHkJCBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "# create a k-nn classifier with 3 neighbors: knn\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# fit the classifier to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy: {0}\".format(knn.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "EOO8GyoUI_us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Varying Number of Neighbours"
      ],
      "metadata": {
        "id": "U3hCxELjKJ1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setyp arrays to store train and test accuracies\n",
        "neighbors = np.arange(1, 9)\n",
        "train_accuracy = np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "\n",
        "# loop over different values of k\n",
        "for i, k in enumerate(neighbors):\n",
        "  # setup a k-nn clasifier with k neighbors: knn\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "  # fit the classifier on the training set\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  #compute accuracy on the training set\n",
        "  train_accuracy[i] = knn.score(X_train, y_train)\n",
        "\n",
        "  # compute accuracy an the testing set\n",
        "  test_accuracy[i] = knn.score(X_test, y_test)\n",
        "\n",
        "# generate plot\n",
        "plt.title(\"k-NN varying number of Neighbors\")\n",
        "plt.plot(neighbors, test_accuracy, label = \"Testing Accuracy\")\n",
        "plt.plot(neighbors, train_accuracy, label = \"Training Accuracy\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Nuber of Neifhbors\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()  "
      ],
      "metadata": {
        "id": "7dgTHH7gKKuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification with deep learning"
      ],
      "metadata": {
        "id": "GwBvPS7eNwtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n"
      ],
      "metadata": {
        "id": "m0p2lZdnNwfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "mnist = datasets.MNIST(root='.', train=True, download=True)"
      ],
      "metadata": {
        "id": "htVYFfX3Ocab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training examples\", mnist.train_data.shape)\n",
        "print(\"Image information\", mnist[0])"
      ],
      "metadata": {
        "id": "If1nmniAPGjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(mnist[0][0])"
      ],
      "metadata": {
        "id": "tq2nRIoKP-uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__ (self):\n",
        "        super(Net, self) .__init__()\n",
        "        \n",
        "        self.fully = nn.Sequential(\n",
        "        nn.Linear(28*28, 10))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view([-1, 28*28])\n",
        "        x = self.fully(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "So4sRBsvQUL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST(root='.', train= True, transform=transforms.Compose([transforms.\n",
        "ToTensor()])), batch_size = 64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST(root='.', train= False, transform=transforms.Compose([transforms.\n",
        "ToTensor()])), batch_size = 1, shuffle=True)"
      ],
      "metadata": {
        "id": "Ovog8F1oGNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train() :\n",
        "    learning_rate = 1e-3\n",
        "    num_epochs = 3\n",
        "    \n",
        "    net = Net()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            output = net(data)\n",
        "            \n",
        "            loss = F.nll_loss(output, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                print('Epoch = %f.Batch = %s. Loss = %s' % (epoch, batch_idx, loss.item()))\n",
        "            \n",
        "    return net"
      ],
      "metadata": {
        "id": "qiEyAYNoRekt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = train()"
      ],
      "metadata": {
        "id": "7r7oFr29VGkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data, target in test_loader:\n",
        "    total += len(target)\n",
        "    output = net(data)\n",
        "    pred = output.max(1, keepdim= True)[1]\n",
        "    correct += target.eq(pred.view_as(target)).sum()\n",
        "    \n",
        "print(\"Correct out of %s\" % total, correct.item())\n",
        "print(\"Percentage accuracy\", correct.item()*100/10000.)"
      ],
      "metadata": {
        "id": "CM_T3c3gW6VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. Linear Regression"
      ],
      "metadata": {
        "id": "vf6AJNsdO0uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❖ Load the dataset"
      ],
      "metadata": {
        "id": "t6gJPdcPO6DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy anf pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read file csv into a dataframe : df\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/gapminder.csv\")"
      ],
      "metadata": {
        "id": "MY6sjKNWOw4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(df.corr(), square=True, cmap=\"RdYlGn\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lfqns_63P6g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❖ Apply linear regression with the 'fertility' feature to predict life expectancy"
      ],
      "metadata": {
        "id": "O4OmBsrAQbw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_fertility = df['fertility'].values.reshape(-1, 1)\n",
        "y_life = df['life'].values.reshape(-1, 1)\n",
        "prediction_space = np.linspace(min(x_fertility), max(x_fertility), num=100).reshape(-1, 1)\n",
        "# create training and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_fertility, y_life, test_size=0.3, random_state=42)\n",
        "\n",
        "# create the regression model: reg_all\n",
        "reg = LinearRegression()\n",
        "\n",
        "# fit the regression to the training data\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "y_predict = reg.predict(prediction_space)\n",
        "\n",
        "# print accuracy\n",
        "print(reg.score(x_fertility, y_life))\n",
        "\n",
        "# plot regression line\n",
        "plt.scatter(x_fertility, y_life, color=\"blue\")\n",
        "plt.plot(prediction_space, y_predict, color=\"black\", linewidth=3)\n",
        "plt.ylabel('Life Expectancy')\n",
        "plt.xlabel('Fertility')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8syJaY0QX29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❖ Apply linear regression with the all features to predict life expectancy. Compare the model score when\n",
        "using all features to one feature in previous step."
      ],
      "metadata": {
        "id": "tppuWHA-WnpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/gapminder.csv\")\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/gapminder.csv\")\n",
        "del features['life']\n",
        "del features['Region']\n",
        "\n",
        "y_life = df['life'].values.reshape(-1, 1)\n",
        "\n",
        "# create training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, y_life, test_size=0.3, random_state=42)\n",
        "\n",
        "#create the regression model: reg_all\n",
        "reg_all = LinearRegression()\n",
        "\n",
        "#fit the regression to the training data\n",
        "reg_all.fit(x_train, y_train)\n",
        "\n",
        "# print accuracy\n",
        "print(reg_all.score(features, y_life))"
      ],
      "metadata": {
        "id": "2ta3l7B9WgCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "sm7lCb6EYeA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "m = .9\n",
        "c = 1\n",
        "x = np.linspace(0, 2*np.pi, N)\n",
        "y = m*x + c + np.random.normal(0, .3,x.shape)\n",
        "plt.figure()\n",
        "plt.plot(x, y, 'o')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('2D data (# data = %d' % N)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ABqsO2jxZYbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'feature': torch.tensor([1, self.x[idx]]),\n",
        "            'label': torch.tensor([self.y[idx]])\n",
        "        }\n",
        "        return sample\n",
        "    "
      ],
      "metadata": {
        "id": "jireFmX_aJVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create example data\n",
        "x = torch.randn(10)\n",
        "y = torch.randint(0, 2, (10, ))\n",
        "\n",
        "# create dataset\n",
        "dataset = MyDataset(x, y)\n",
        "\n",
        "# loop over dataset and print feature and label values\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]\n",
        "    print(i, sample['feature'], sample['label'])"
      ],
      "metadata": {
        "id": "CqlGVFapbVOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = MyDataset(x, y)\n",
        "\n",
        "batch_size = 4\n",
        "shuffle = True\n",
        "num_workers = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "p77YuW-vcrTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "TdfF745I3_Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint as pp\n",
        "\n",
        "for i_batch, samples in enumerate(dataloader):\n",
        "  print('\\nbatch# = %s' % i_batch)\n",
        "  print('samples: ')\n",
        "  pp.pprint(samples)"
      ],
      "metadata": {
        "id": "Eedj6XpydqU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "WFBjsZmS5fC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.linear = nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    out = self.linear(x)\n",
        "    return out  "
      ],
      "metadata": {
        "id": "oQQvm4kqeVjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting a model for our problem"
      ],
      "metadata": {
        "id": "rUsWx3WX5i1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "model = MyModel(input_dim, output_dim)"
      ],
      "metadata": {
        "id": "7cCZIA8hfR_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost dfunction\n"
      ],
      "metadata": {
        "id": "CXHjFz__5nEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = nn.MSELoss()"
      ],
      "metadata": {
        "id": "geH35TcqfiL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimizig the cost function"
      ],
      "metadata": {
        "id": "VhhU_igB5sZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tables import parameters\n",
        "# from scipy.optimize import optimize\n",
        "num_epochs = 10\n",
        "l_rate = 0.01\n",
        "\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate)\n",
        "\n",
        "dataset = MyDataset(x, y)\n",
        "batch_size = 4\n",
        "shuffle = True\n",
        "num_workers = 0\n",
        "training_sample_generator = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch = %s' % epoch)\n",
        "    for batch_i, samples in enumerate(training_sample_generator):\n",
        "        predictions = model(samples['feature'].float())\n",
        "        error = cost(predictions, samples['label'].float())\n",
        "        print('\\tBatch = %s, Error = %s' % (batch_i, error.item()))\n",
        "        optimiser.zero_grad()\n",
        "        error.backward()\n",
        "        optimiser.step()"
      ],
      "metadata": {
        "id": "UU16QYYNfpjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see how well the model has learnt the data"
      ],
      "metadata": {
        "id": "cDJzYYjw7Ogs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_for_plotting = np.linspace(0, 2*np.pi, 1000)\n",
        "design_matrix = torch.tensor(np.vstack([np.ones(x_for_plotting.shape), x_for_plotting]).T,dtype = torch.float32)\n",
        "print(\"Design matrix shape:\", design_matrix.shape)\n",
        "\n",
        "y_for_plotting = model.forward(design_matrix)\n",
        "print('y_for_plotting shape: ', y_for_plotting.shape)"
      ],
      "metadata": {
        "id": "nck_Zz0Jh-wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(x, y, 'o')\n",
        "plt.plot(x_for_plotting, y_for_plotting.data.numpy(), 'r-')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('2D data (#data = %d)' % N)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sqTJXNfPjTYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#III. Recommendation Systems"
      ],
      "metadata": {
        "id": "ci3wQZyz7eON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def user_cf(M, metric=\"cosine\"):\n",
        "    pred = np.copy(M)\n",
        "    n_users, n_items = M.shape\n",
        "    avg_ratings = np.nanmean(M, axis=1)\n",
        "    sim_users = sim_matrix(M, 'user', metric)\n",
        "    for i in range(n_users):\n",
        "        for j in range(n_items):\n",
        "            if np.isnan(M[i, j]):\n",
        "                pred[i, j] = avg_ratings[i] + np.nansum(sim_users[i] * (M[:, j] - avg_ratings)) / sum(sim_users[i])\n",
        "    return pred    \n",
        "\n",
        "def item_cf(M, metric='cosine'):\n",
        "    pred = np.copy(M)\n",
        "    n_users, n_items = M.shape\n",
        "    avg_ratings = np.nanmean(M, axis=0)\n",
        "    sim_items = sim_matrix(M, 'item', metric)\n",
        "    for i in range(n_users):\n",
        "        for j in range(n_items):\n",
        "            if np.isnan(M[i, j]):\n",
        "                pred[i, j] = avg_ratings[j] + np.nansum(sim_items[j] * (M[i,:] - avg_ratings)) / sum(sim_items[j])\n",
        "    return pred    \n",
        "\n",
        "def sim_matrix(M, matrix_type, metric):\n",
        "    if matrix_type == 'user':\n",
        "        M = M.T\n",
        "    sim = np.zeros((M.shape[0], M.shape[0]))\n",
        "    for i in range(M.shape[0]):\n",
        "        for j in range(i, M.shape[0]):\n",
        "            if i == j:\n",
        "                sim[i,j] = 1\n",
        "            else:\n",
        "                if metric == 'cosine':\n",
        "                    sim[i,j] = np.dot(M[i,:], M[j,:]) / (np.linalg.norm(M[i,:]) * np.linalg.norm(M[j,:]))\n",
        "                elif metric == 'correlation':\n",
        "                    sim[i,j] = np.corrcoef(M[i,:], M[j,:])[0,1]\n",
        "                sim[j,i] = sim[i,j]\n",
        "    if matrix_type == 'user':\n",
        "        sim = sim.T\n",
        "    return sim\n",
        "\n",
        "def evaluateRs(M, M_result, method, metric):\n",
        "    if method == 'user_cf':\n",
        "        M_pred = user_cf(M, metric)\n",
        "    elif method == 'item_cf':\n",
        "        M_pred = item_cf(M, metric)\n",
        "    else:\n",
        "        print(\"Invalid method.\")\n",
        "        return\n",
        "    idx = np.argwhere(np.isnan(M))\n",
        "    RMSE = np.sqrt(np.nanmean((M[idx[:,0], idx[:,1]] - M_pred[idx[:,0], idx[:,1]])**2))\n",
        "    print(\"RMSE for %s with %s similarity: %f\" % (method, metric, RMSE))\n",
        "\n",
        "# Example usage\n",
        "M = np.array([[1, 2, np.nan], [3, np.nan, 4], [5, 6, 7]])\n",
        "M_result = np.array([[1, 2, 3], [3, 5, 4], [5, 6, 7]])\n",
        "evaluateRs(M, M_result, \"user_cf\", \"cosine\")\n",
        "evaluateRs(M, M_result, \"user_cf\", \"correlation\")\n",
        "evaluateRs(M, M_result, \"item_cf\", \"cosine\")\n",
        "evaluateRs(M, M_result, \"item_cf\", \"correlation\")"
      ],
      "metadata": {
        "id": "DviLQZ0R8C3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rank(M, M_result, method, metric):\n",
        "    results = []\n",
        "    for method in ['user_cf', 'item_cf']:\n",
        "        for metric in ['cosine', 'correlation']:\n",
        "            rank_acc = evaluate_rank(M, M_result, method, metric)\n",
        "            results += [\"Rank accuracy of {0} with {1} metirc: {2}\".format(method[1], metric, rank_acc)]\n",
        "\n",
        "    print(\"\\n\".join(results))\n",
        "pass"
      ],
      "metadata": {
        "id": "atz1yO_dk8zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IV. Exercises"
      ],
      "metadata": {
        "id": "YsEpmdTzpAuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y4sLS1_O8ZKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "iris = load_iris()\n",
        "iris"
      ],
      "metadata": {
        "id": "K2-XgWmLpCnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = iris.data\n",
        "Y =  iris.target\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zq7DEtz18bl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "knn.fit(X_train, Y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "acc = knn.score(X_test, Y_test)\n",
        "\n",
        "y_pred_prob = knn.predict_proba(X_test)\n",
        "loss = log_loss(Y_test, y_pred_prob)\n",
        "print(\"Test loss:\", loss)\n",
        "\n",
        "print(\"Test accuracy:\", acc)"
      ],
      "metadata": {
        "id": "wC6Wwyd98ggc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighbors = np.arange(1,9)\n",
        "train_accuracy = np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "\n",
        "for i, k in enumerate(neighbors):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    knn.fit(X_train, Y_train)\n",
        "    train_accuracy[i] = knn.score(X_train, Y_train)\n",
        "    test_accuracy[i] = knn.score(X_test, Y_test)\n",
        "\n",
        "    print(f\"{i+1}/{len(neighbors)} [{'='*int(test_accuracy[i]*20):<20}] - {test_accuracy[i]:.4f} - k: {k}\")\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy[-1]:.4f}\")\n",
        "\n",
        "plt.title('k-NN: Varying Number of Neighbors')\n",
        "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
        "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nkmm2XtD8h9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# One-hot encode target variable\n",
        "enc = OneHotEncoder()\n",
        "y = enc.fit_transform(y.reshape(-1,1)).toarray()\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_dim=4, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "x9iaEi9x8kdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a simple neural network with one hidden layer of 10 nodes\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the training data\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print('Test accuracy:', test_accuracy)\n",
        "\n",
        "# Plot the training and validation accuracy over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ibuLQiSd8tyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DztOIfcUI-h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the data to match the input shape of a CNN\n",
        "X_train = X_train.reshape(-1, 4, 1, 1)\n",
        "X_test = X_test.reshape(-1, 4, 1, 1)"
      ],
      "metadata": {
        "id": "NoURKWJ6JBvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=(2, 1), activation='relu', input_shape=(4, 1, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "Ek5eoqYSJDXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the testing set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Visualize the training and validation accuracy and loss over time\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "lyo7jgiTJHw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ICDizHQ_JRKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Recommendation Systems"
      ],
      "metadata": {
        "id": "Moi45abeJY26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/movies.csv', 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(100000))\n",
        "result\n"
      ],
      "metadata": {
        "id": "rxTQa9jrJZhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "movies = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/movies.csv',encoding='ISO-8859-1')\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/ratings.csv')\n",
        "users = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/users.csv')\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "i1KzivZKJs2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(movies.genres))"
      ],
      "metadata": {
        "id": "ZRKAmA4OJ5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert float values in the 'genres' column to empty strings\n",
        "movies['genres'] = movies['genres'].apply(lambda x: '' if isinstance(x, float) else x)\n",
        "\n",
        "# Split the genre strings into lists of genres\n",
        "movies['genres'] = movies['genres'].apply(lambda x: x.split('|') if isinstance(x, str) else [])\n",
        "\n",
        "# Create a MultiLabelBinarizer object to binarize the genres\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Binarize the genres for each movie and create a new dataframe with the binary vectors\n",
        "genres = pd.DataFrame(mlb.fit_transform(movies['genres']), columns=mlb.classes_, index=movies.index)\n",
        "\n",
        "# Select the subset of the binary vectors corresponding to the genres\n",
        "Ij = genres.values\n",
        "\n",
        "# Print the first four rows of Ij\n",
        "print(Ij[:4])"
      ],
      "metadata": {
        "id": "l2eG5j7QJ-oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/ratings.csv')\n",
        "movies = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/movies.csv',encoding='ISO-8859-1')\n",
        "\n",
        "# Assume we have a list of dictionaries called 'ratings' where each dictionary\n",
        "# represents a user's ratings for different movies\n",
        "ratings_list = []\n",
        "for _, row in ratings.iterrows():\n",
        "    rating_dict = {'userId': row['user_id'], 'movieId': row['movie_id'], 'rating': row['rating']}\n",
        "    ratings_list.append(rating_dict)\n",
        "\n",
        "# Convert the list of dictionaries to a numpy array\n",
        "ratings_matrix = np.array([list(r.values()) for r in ratings_list])\n",
        "\n",
        "# Compute cosine similarity between all pairs of rows in the matrix\n",
        "user_similarity = cosine_similarity(ratings_matrix)\n",
        "\n",
        "# Assume we have a matrix called 'movies_matrix' where each row represents a movie\n",
        "# and each column represents a feature of that movie (e.g. genre, director, etc.)\n",
        "\n",
        "# Convert movie genres to numerical values\n",
        "genres = set()\n",
        "for genre_string in movies['genres'].values:\n",
        "    if not isinstance(genre_string, str):  # check if the value is not a string\n",
        "        continue\n",
        "    genres.update(genre_string.split('|'))\n",
        "\n",
        "genre_dict = {genre: i for i, genre in enumerate(sorted(genres))}\n",
        "movies_matrix = np.zeros((movies.shape[0], len(genres)))\n",
        "for i, genre_string in enumerate(movies['genres'].values):\n",
        "    if not isinstance(genre_string, str):  # check if the value is not a string\n",
        "        continue\n",
        "    for genre in genre_string.split('|'):\n",
        "        movies_matrix[i, genre_dict[genre]] = 1\n",
        "\n",
        "# Compute cosine similarity between all pairs of rows in the matrix\n",
        "movie_similarity = cosine_similarity(movies_matrix)"
      ],
      "metadata": {
        "id": "KDuidA3_KDPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_similarity)"
      ],
      "metadata": {
        "id": "LK5G8SC4KKw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/ratings.csv')\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.5, random_state=42)\n",
        "\n",
        "# Calculate ratings using the training dataset\n",
        "ratings = train_ratings.groupby(['user_id', 'movie_id'])['rating'].mean().reset_index()\n",
        "\n",
        "# Print the ratings for the first 10 users and products\n",
        "print(ratings.head(10))"
      ],
      "metadata": {
        "id": "CWYvFiYOKN-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/ratings.csv')\n",
        "\n",
        "# Split the data into training and testing datasets\n",
        "train_data = ratings.sample(frac=0.8, random_state=42)\n",
        "test_data = ratings.drop(train_data.index)\n",
        "\n",
        "# Create matrix for training dataset\n",
        "train_data_matrix = train_data.pivot_table(index='user_id', columns='movie_id', values='rating').astype('float64')\n",
        "\n",
        "# Create matrix for testing dataset\n",
        "test_data_matrix = test_data.pivot_table(index='user_id', columns='movie_id', values='rating').astype('float64')\n",
        "\n",
        "# Print the shape of the matrices\n",
        "print('Training dataset matrix shape:', train_data_matrix.shape)\n",
        "print('Testing dataset matrix shape:', test_data_matrix.shape)"
      ],
      "metadata": {
        "id": "TYjWY3wgKUHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}