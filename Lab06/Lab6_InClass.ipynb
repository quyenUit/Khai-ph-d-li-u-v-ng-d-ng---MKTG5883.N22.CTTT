{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nguyễn Hữu Quyến - 19522113\n",
        "\n",
        "Lab6\n",
        "\n",
        "link: "
      ],
      "metadata": {
        "id": "fFv94QT2Yaxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`InClass`**"
      ],
      "metadata": {
        "id": "zkWppZ8CWiQ0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK5LVC15NjO7"
      },
      "source": [
        "# Lab 7 - Textual Data Analytics\n",
        "Complete the code with TODO tag.\n",
        "## 1. Feature Engineering\n",
        "In this exercise we will understand the functioning of TF/IDF ranking. Implement the feature engineering and its application, based on the code framework provided below.\n",
        "\n",
        "First we use textual data from Twitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "2XX5m4NDNjPs",
        "outputId": "1b925019-d805-4dac-e3d4-5fb7c701f959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2819\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   id           created_at  \\\n",
              "0  849636868052275200  2017-04-05 14:56:29   \n",
              "1  848988730585096192  2017-04-03 20:01:01   \n",
              "2  848943072423497728  2017-04-03 16:59:35   \n",
              "3  848935705057280001  2017-04-03 16:30:19   \n",
              "4  848416049573658624  2017-04-02 06:05:23   \n",
              "\n",
              "                                                text  \n",
              "0  b'And so the robots spared humanity ... https:...  \n",
              "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...  \n",
              "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'  \n",
              "3                b'Stormy weather in Shortville ...'  \n",
              "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae4765c0-bf07-488f-9483-c9b3577d3739\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>849636868052275200</td>\n",
              "      <td>2017-04-05 14:56:29</td>\n",
              "      <td>b'And so the robots spared humanity ... https:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>848988730585096192</td>\n",
              "      <td>2017-04-03 20:01:01</td>\n",
              "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>848943072423497728</td>\n",
              "      <td>2017-04-03 16:59:35</td>\n",
              "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>848935705057280001</td>\n",
              "      <td>2017-04-03 16:30:19</td>\n",
              "      <td>b'Stormy weather in Shortville ...'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>848416049573658624</td>\n",
              "      <td>2017-04-02 06:05:23</td>\n",
              "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae4765c0-bf07-488f-9483-c9b3577d3739')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae4765c0-bf07-488f-9483-c9b3577d3739 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae4765c0-bf07-488f-9483-c9b3577d3739');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Data_Mining/Thuc_hanh/notebooks and data/elonmusk_tweets.csv')\n",
        "print(len(data))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWKsDYSvOdS4",
        "outputId": "3daadebd-f816-4708-a825-4956975b23e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C7YbY3iNjP8"
      },
      "source": [
        "### 1.1. Text Normalization\n",
        "Now we need to normalize text by stemming, tokenizing, and removing stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyqX_aH5NjQE",
        "outputId": "fcbaf6df-3e36-44b3-c96a-17d43462c3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "import pprint \n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpmBuLQzNjQH",
        "outputId": "b0688e85-42b7-4fe6-e786-9bed3bf870dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['band', 'so', 'the', 'robot', 'spare', 'human', 'httpstcov7jujqwfcv']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def normalize(document):\n",
        "    # TODO: remove punctuation\n",
        "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
        "    # TODO: tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    \n",
        "    # TODO: Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    ret = \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
        "    return ret\n",
        "\n",
        "original_documents = [x.strip() for x in data['text']] \n",
        "documents = [normalize(d).split() for d in original_documents]\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KhxLUXoNjQK"
      },
      "source": [
        "As you can see that the normalization is still not perfect. Please feel free to improve upon (OPTIONAL), e.g. https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfS790fZNjQL"
      },
      "source": [
        "### 1.2. Implement TF-IDF\n",
        "Now you need to implement TF-IDF, including creating the vocabulary, computing term frequency, and normalizing by tf-idf weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63HkNjrfNjQM",
        "outputId": "19fdcf40-ff71-460c-df9a-3de742b9c35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tesla', 287)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brt', 'tesla', 'spacex', 'model', 'thi']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Flatten all the documents\n",
        "flat_list = [word for doc in documents for word in doc]\n",
        "\n",
        "# TODO: remove stop words from the vocabulary\n",
        "words = [word for word in flat_list if word not in stopwords.words('english')]\n",
        "\n",
        "# TODO: we take the 500 most common words only\n",
        "counts = Counter(words)\n",
        "vocabulary = counts.most_common(500)\n",
        "print([x for x in vocabulary if x[0] == 'tesla'])\n",
        "vocabulary = [x[0] for x in vocabulary]\n",
        "assert len(vocabulary) == 500\n",
        "\n",
        "# vocabulary.sort()\n",
        "vocabulary[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S8vczMZNjQO",
        "outputId": "fe6191e0-3487-4d13-8bc8-2c94f6cd7b49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['tesla', 'exactli'], dtype='<U17'), array([1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def tf(vocabulary, documents):\n",
        "    matrix = [0] * len(documents)\n",
        "    for i, document in enumerate(documents):\n",
        "        counts = Counter(document)\n",
        "        matrix[i] = [0] * len(vocabulary)\n",
        "        for j, term in enumerate(vocabulary):\n",
        "            matrix[i][j] = counts[term]\n",
        "    return matrix\n",
        "\n",
        "tf = tf(vocabulary, documents)\n",
        "np.array(vocabulary)[np.where(np.array(tf[1]) > 0)], np.array(tf[1])[np.where(np.array(tf[1]) > 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wCNP4cENjQR",
        "outputId": "14921247-5749-4299-f224-3264030cbe4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.539126825495932,\n",
              " 3.3163095197385393,\n",
              " 3.7262581423445837,\n",
              " 3.8171115727956972,\n",
              " 3.8027562798186274]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def idf(vocabulary, documents):\n",
        "    \"\"\"TODO: compute IDF, storing values in a dictionary\"\"\"\n",
        "    idf = {}\n",
        "    num_documents = len(documents)\n",
        "    for i, term in enumerate(vocabulary):\n",
        "      idf[term] = math.log(num_documents / sum(term in document for document in documents), 2)\n",
        "    return idf\n",
        "\n",
        "idf = idf(vocabulary, documents)\n",
        "[idf[key] for key in vocabulary[:5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJf2DOlTNjQS",
        "outputId": "564863c6-2b9e-49c3-f574-4a31ce8dd85f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['tesla', 'exactli'], dtype='<U17'), array([3.31630952, 6.65361284]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def vectorize(document, vocabulary, idf):\n",
        "    vector = [0]*len(vocabulary)\n",
        "    counts = Counter(document)\n",
        "    for i,term in enumerate(vocabulary):\n",
        "        vector[i] = idf[term] * counts[term]\n",
        "    return vector\n",
        "\n",
        "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
        "np.array(vocabulary)[np.where(np.array(document_vectors[1]) > 0)], np.array(document_vectors[1])[np.where(np.array(document_vectors[1]) > 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jxaY3flNjQT"
      },
      "source": [
        "### 1.3. Compare the results with the reference implementation of scikit-learn library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr6HNuadNjQU"
      },
      "source": [
        "Now we use the scikit-learn library. As you can see that, the way we do text normalization affects the result. Feel free to further improve upon (OPTIONAL), e.g. https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD7Mqb-eNjQU",
        "outputId": "3b3ac791-f2b2-4c3b-99fa-163c79e4bf1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('http', 163.54366542841234), ('https', 151.85039944652075), ('rt', 112.61998731390989), ('tesla', 95.96401470715628), ('xe2', 88.20944486346477)]\n",
            "testla 0.3495243100660956\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english', max_features=500)\n",
        "\n",
        "features = tfidf.fit(original_documents)\n",
        "corpus_tf_idf = tfidf.transform(original_documents) \n",
        "\n",
        "sum_words = corpus_tf_idf.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
        "print(sorted(words_freq, key = lambda x: x[1], reverse=True)[:5])\n",
        "print('testla', corpus_tf_idf[1, features.vocabulary_['tesla']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuER1W8pNjQV"
      },
      "source": [
        "### 1.4.  Apply TF-IDF for information retrieval\n",
        "We can use the vector representation of documents to implement an information retrieval system. We test with the query $Q$ = \"tesla nasa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dJHV4LqNjQW",
        "outputId": "0fc91d3d-23b5-4ed1-dee4-0a38651248e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-5 documents\n",
            "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
            "1 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n",
            "2 b\"Deeply appreciate @NASA's faith in @SpaceX. We will do whatever it takes to make NASA and the American people proud.\"\n",
            "3 b'Would also like to congratulate @Boeing, fellow winner of the @NASA commercial crew program'\n",
            "4 b\"@astrostephenson We're aiming for late 2015, but NASA needs to have overlapping capability to be safe. Would do the same\"\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(v1,v2):\n",
        "    \"\"\"TODO: compute cosine similarity\"\"\"\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "      x = v1[i]; y = v2[i]\n",
        "      sumxx += x*x\n",
        "      sumyy += y*y\n",
        "      sumxy += x*y\n",
        "    if sumxy == 0:\n",
        "      result = 0\n",
        "    else:\n",
        "      result = sumxy/math.sqrt(sumxx*sumyy)    \n",
        "    return result\n",
        "\n",
        "def search_vec(query, k, vocabulary, stemmer, document_vectors, original_documents):\n",
        "    q = query.split()\n",
        "    q = [stemmer.stem(w) for w in q]\n",
        "    query_vector = vectorize(q, vocabulary, idf)\n",
        "    \n",
        "    # TODO: rank the documents by cosine similarity\n",
        "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(document_vectors))]\n",
        "    scores.sort(key=lambda x : -x[0])\n",
        "    \n",
        "    print('Top-{0} documents'.format(k))\n",
        "    for i in range(k):\n",
        "        print(i, original_documents[scores[i][1]])\n",
        "\n",
        "query = \"tesla nasa\"\n",
        "stemmer = PorterStemmer()\n",
        "search_vec(query, 5, vocabulary, stemmer, document_vectors, original_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6OOx8OyNjQX"
      },
      "source": [
        "We can also use the scikit-learn library to do the retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iK6auXqNjQX",
        "outputId": "46e3d59b-bdcd-4d89-cc88-338dade20549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-5 documents\n",
            "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
            "1 b\"SpaceX could not do this without NASA. Can't express enough appreciation. https://t.co/uQpI60zAV7\"\n",
            "2 b'@NASA launched a rocket into the northern lights http://t.co/tR2cSeMV'\n",
            "3 b'Whatever happens today, we could not have done it without @NASA, but errors are ours alone and me most of all.'\n",
            "4 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n"
          ]
        }
      ],
      "source": [
        "new_features = tfidf.transform([query])\n",
        "\n",
        "cosine_similarities = linear_kernel(new_features, corpus_tf_idf).flatten()\n",
        "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
        "\n",
        "topk = 5\n",
        "print('Top-{0} documents'.format(topk))\n",
        "for i in range(topk):\n",
        "    print(i, original_documents[related_docs_indices[i]])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}